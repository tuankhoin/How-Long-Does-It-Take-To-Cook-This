{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFpr, chi2, mutual_info_classif\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "from collections import defaultdict,Counter\n",
    "from text_to_num import text2num,alpha2digit\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file, \n",
    "               process = None, \n",
    "               convert_to_seconds = False):\n",
    "    curr_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    if not process:\n",
    "        train_file = os.path.join(curr_dir, 'COMP30027_2021_Project2_datasets\\\\'+file)\n",
    "    else:\n",
    "        train_file = os.path.join(curr_dir, 'COMP30027_2021_Project2_datasets\\\\recipe_text_features_' + process + '\\\\' + file)\n",
    "\n",
    "    data = pd.read_csv(train_file, index_col = False, delimiter = ',')\n",
    "    if convert_to_seconds:\n",
    "        tqdm.pandas(desc=\"Converting...\")\n",
    "        data['seconds'] = data['steps'].progress_apply(convert_step_to_time)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_retrieval = lambda words, sentence : re.findall(re.compile('|'.join(\n",
    "        [n + w + \"\\\\b\" for w in words for n in [\"[0-9]+\\s*\",\"another \",\"few \",\"a \",\"an \",\"several \"]])), sentence)\n",
    "\n",
    "kw_retrieval = lambda words, sentence : re.findall(re.compile('|'.join(\n",
    "        [\"\\\\b\" + w + \"\\\\b\" for w in words])), sentence)\n",
    "\n",
    "def convert_step_to_time(step, keyword_check = True):\n",
    "    total_time = 0 # in seconds\n",
    "    times = defaultdict(list)\n",
    "    time_unit = {\"second\":1, \"minute\":60, \"hour\":60*60}\n",
    "    kw_dict = {\"overnight\":8, \"night\":8, \"nights\":8, \"freeze\":5, \"refrigerate\":3,\n",
    "               \"day\":12, \"cook on low\":7, \"slow cook\":7, \"crockpot\":7, \"crock pot\":7,\n",
    "               \"cook low\":7, \"boil\":1, \"heat\":1, \"bread machine\":1.5\n",
    "               }\n",
    "    \n",
    "    # Iterate through each step to find time value using RE\n",
    "    numeric_step = alpha2digit(step,'en')\n",
    "    times[\"second\"] = time_retrieval([\"more seconds\",\"more second\",\"more secs\",\"more sec\",\"seconds\",\"second\",\"secs\",\"sec\",\"s\"],numeric_step)\n",
    "    times[\"minute\"] = time_retrieval([\"more minutes\",\"more minute\",\"more min\",\"more mins\",\"minutes\",\"minute\",\"min\",\"mins\",\"m\",\"ms\"],numeric_step)\n",
    "    times[\"hour\"] = time_retrieval([\"more hours\",\"more hour\",\"more hrs\",\"more hr\",\"hours\",\"hour\",\"hrs\",\"hr\",\"h\",\"hs\"],numeric_step)\n",
    "    special_keywords = kw_retrieval(kw_dict.keys(),numeric_step) if keyword_check else None\n",
    "\n",
    "    for unit in time_unit.keys():\n",
    "        #total_time += sum([float(re.findall(r'[0-9]+',t.split()[0])[0]) * time_unit[unit] for t in times[unit]])\n",
    "        count = 0\n",
    "        for t in times[unit]:\n",
    "            numerator = t.split()[0]\n",
    "\n",
    "            if numerator in [\"another\",\"a\",\"an\"]:\n",
    "                count = 1\n",
    "            elif numerator in [\"few\",\"several\"]:\n",
    "                count = 4\n",
    "            else:\n",
    "                count = float(re.findall(r'[0-9]+',numerator)[0])\n",
    "\n",
    "            total_time += count * time_unit[unit]\n",
    "\n",
    "    if keyword_check and not total_time and special_keywords:\n",
    "        total_time += 60*60*kw_dict[special_keywords[0]]\n",
    "\n",
    "    if total_time == 0:\n",
    "        return None\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_retrieval = lambda words, sentence : re.findall(re.compile('|'.join(\n",
    "        [n + w + \"\\\\b\" for w in words for n in [\"[0-9]+\\s*\",\"another \",\"few \",\"a \",\"an \",\"several \"]])), sentence)\n",
    "\n",
    "kw_retrieval = lambda words, sentence : re.findall(re.compile('|'.join(\n",
    "        [\"\\\\b\" + w + \"\\\\b\" for w in words])), sentence)\n",
    "\n",
    "def convert_step_to_time(step, keyword_check = True):\n",
    "    total_time = 0 # in seconds\n",
    "    times = defaultdict(list)\n",
    "    time_unit = {\"second\":1, \"minute\":60, \"hour\":60*60}\n",
    "    kw_dict = {\"overnight\":8, \"night\":8, \"nights\":8, \"freeze\":5, \"refrigerate\":3,\n",
    "               \"day\":12, \"cook on low\":7, \"slow cook\":7, \"crockpot\":7, \"crock pot\":7,\n",
    "               \"cook low\":7, \"boil\":1, \"heat\":1, \"bread machine\":1.5\n",
    "               }\n",
    "    \n",
    "    # Iterate through each step to find time value using RE\n",
    "    numeric_step = alpha2digit(step,'en')\n",
    "    times[\"second\"] = time_retrieval([\"more seconds\",\"more second\",\"more secs\",\"more sec\",\"seconds\",\"second\",\"secs\",\"sec\",\"s\"],numeric_step)\n",
    "    times[\"minute\"] = time_retrieval([\"more minutes\",\"more minute\",\"more min\",\"more mins\",\"minutes\",\"minute\",\"min\",\"mins\",\"m\",\"ms\"],numeric_step)\n",
    "    times[\"hour\"] = time_retrieval([\"more hours\",\"more hour\",\"more hrs\",\"more hr\",\"hours\",\"hour\",\"hrs\",\"hr\",\"h\",\"hs\"],numeric_step)\n",
    "    special_keywords = kw_retrieval(kw_dict.keys(),numeric_step) if keyword_check else None\n",
    "\n",
    "    for unit in time_unit.keys():\n",
    "        #total_time += sum([float(re.findall(r'[0-9]+',t.split()[0])[0]) * time_unit[unit] for t in times[unit]])\n",
    "        count = 0\n",
    "        for t in times[unit]:\n",
    "            numerator = t.split()[0]\n",
    "\n",
    "            if numerator in [\"another\",\"a\",\"an\"]:\n",
    "                count = 1\n",
    "            elif numerator in [\"few\",\"several\"]:\n",
    "                count = 4\n",
    "            else:\n",
    "                count = float(re.findall(r'[0-9]+',numerator)[0])\n",
    "\n",
    "            total_time += count * time_unit[unit]\n",
    "\n",
    "    if keyword_check and not total_time and special_keywords:\n",
    "        total_time += 60*60*kw_dict[special_keywords[0]]\n",
    "\n",
    "    if total_time == 0:\n",
    "        return None\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('COMP30027_2021_Project2_datasets\\\\recipe_train.csv')\n",
    "train_data_50 = pd.read_csv('COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec50\\\\train_steps_doc2vec50.csv',header=None, names = ['coord'+str(i) for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_data,train_data_50],axis=1)\n",
    "y = train['duration_label']\n",
    "x = train.drop(['name','steps','ingredients','duration_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process takes  0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1, verbose=True)\n",
    "c2 = GaussianNB()\n",
    "c3 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1, verbose=False)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1, verbose=False)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True, verbose=False)\n",
    "\n",
    "#sclf.fit(X_train,y_train)\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n         1.0       0.72      0.71      0.71      2646\\n         2.0       0.72      0.77      0.74      3053\\n         3.0       0.49      0.20      0.28       301\\n\\n    accuracy                           0.71      6000\\n   macro avg       0.64      0.56      0.58      6000\\nweighted avg       0.71      0.71      0.71      6000\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.Series(sclf.predict(X_test))\n",
    "classification_report(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.32%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {100*accuracy_score(preds, y_test):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/3)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting classifier2: gaussiannb (2/3)\n",
      "Fitting classifier3: logisticregression (3/3)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/3)\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting classifier2: gaussiannb (2/3)\n",
      "Fitting classifier3: logisticregression (3/3)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/3)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting classifier2: gaussiannb (2/3)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting classifier3: logisticregression (3/3)\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/3)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting classifier2: gaussiannb (2/3)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting classifier3: logisticregression (3/3)\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/3)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Fitting classifier2: gaussiannb (2/3)\n",
      "Fitting classifier3: logisticregression (3/3)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(sclf,x,y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6760249999999999"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(AdaBoostClassifier(),x,y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63335"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(BaggingClassifier(c2),x,y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('COMP30027_2021_Project2_datasets\\\\input\\\\train_seconds.csv')\n",
    "train_data_100 = pd.read_csv('COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\train_steps_doc2vec100.csv',header=None, names = ['coord'+str(i) for i in range(100)])\n",
    "train = pd.concat([train_data,train_data_100],axis=1)\n",
    "y100 = train['duration_label']\n",
    "x100 = train.drop(['name','steps','ingredients','duration_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "y100 = train['duration_label']\n",
    "x100 = train.drop(['name','steps','ingredients','duration_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.6s finished\n",
      "0.7659421488751451\n",
      "Process takes  232.69554495811462 seconds\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7294058455715811"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(SVC(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5699892538439091\n",
      "Process takes  0.5106613636016846 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(GaussianNB(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6779986540110515\n",
      "Process takes  29.413482666015625 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(DecisionTreeClassifier(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP OMEN 15\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\HP OMEN 15\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\HP OMEN 15\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\HP OMEN 15\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "0.6738312460346634\n",
      "Process takes  7.028547763824463 seconds\n",
      "C:\\Users\\HP OMEN 15\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(LogisticRegression(max_iter=10000),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7441915533184241\n",
      "Process takes  138.90527200698853 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(RandomForestClassifier(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7364848804798234\n",
      "Process takes  91.97082996368408 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(AdaBoostClassifier(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7552663885195186\n",
      "Process takes  12.221656560897827 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(KNeighborsClassifier(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7482447571595648\n",
      "Process takes  179.22210359573364 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(BaggingClassifier(DecisionTreeClassifier()),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7566651080659482\n",
      "Process takes  114.75170159339905 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(BaggingClassifier(KNeighborsClassifier()),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7290062410979176\n",
      "Process takes  3350.5638234615326 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(BaggingClassifier(SVC()),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7785866594561183\nProcess takes  1469.8330869674683 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(cross_val_score(GradientBoostingClassifier(),x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process takes  1015.8914716243744 seconds\n",
      "{'n_neighbors': 38}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7703660191201482"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "start = time.time()\n",
    "param_grid = {'n_neighbors': np.arange(38,1000,20)}\n",
    "rcv = GridSearchCV(KNeighborsClassifier(),param_grid, cv=5)\n",
    "rcv.fit(x100,y100)\n",
    "print('Process takes ',time.time()-start, 'seconds')\n",
    "print(rcv.best_params_)\n",
    "rcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675407625166687\n",
      "Process takes  425.40338587760925 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = GaussianNB()\n",
    "c3 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100, cv = 10).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7761890203919485\n",
      "Process takes  291.49292397499084 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = AdaBoostClassifier()\n",
    "c3 = KNeighborsClassifier(n_neighbors=38)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7817265968809544\n",
      "Process takes  246.60619163513184 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "c3 = KNeighborsClassifier(n_neighbors=38)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7719644451629282\n",
      "Process takes  267.93580436706543 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = AdaBoostClassifier()\n",
    "c3 = KNeighborsClassifier(n_neighbors=21)\n",
    "lr = KNeighborsClassifier(n_neighbors=21)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7517270543982342\n",
      "Process takes  276.2685396671295 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = AdaBoostClassifier()\n",
    "c3 = KNeighborsClassifier(n_neighbors=21)\n",
    "lr = BaggingClassifier(DecisionTreeClassifier())\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7691672342175991\n",
      "Process takes  571.7859828472137 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = AdaBoostClassifier()\n",
    "lr = KNeighborsClassifier(n_neighbors=21)\n",
    "c3 = BaggingClassifier(DecisionTreeClassifier())\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7753612074479902\n",
      "Process takes  333.37484431266785 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = AdaBoostClassifier()\n",
    "c3 = KNeighborsClassifier(n_neighbors=38)\n",
    "c4 = KNeighborsClassifier(n_neighbors=38)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3,c4],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7763602899282521\n",
      "Process takes  352.87842202186584 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = AdaBoostClassifier()\n",
    "c3 = KNeighborsClassifier(n_neighbors=38)\n",
    "c4 = DecisionTreeClassifier()\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3,c4],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "start = time.time()\n",
    "print(cross_val_score(sclf,x100,y100).mean())\n",
    "print('Process takes ',time.time()-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting...: 100%|| 10000/10000 [00:11<00:00, 834.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "test_data = preprocess('recipe_test.csv', convert_to_seconds=True)#pd.read_csv('COMP30027_2021_Project2_datasets\\\\recipe_train.csv')\n",
    "test_data_100 = pd.read_csv('COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\test_steps_doc2vec100.csv',header=None, names = ['coord'+str(i) for i in range(100)])\n",
    "test = pd.concat([test_data,test_data_100],axis=1)\n",
    "x100test = test.drop(['name','steps','ingredients'], axis=1)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x100)\n",
    "\n",
    "x100test = imp.transform(x100test)\n",
    "\n",
    "c1 = RandomForestClassifier(n_estimators=100,min_samples_split=50,min_samples_leaf=1, criterion='gini', n_jobs=-1)\n",
    "c2 = GaussianNB()#KNeighborsClassifier(n_neighbors=38)\n",
    "c3 = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000, n_jobs=-1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[c1,c2,c3],\n",
    "                            meta_classifier=lr, use_probas=True)\n",
    "\n",
    "sclf.fit(x100,y100)\n",
    "Y_test = sclf.predict(x100test)\n",
    "\n",
    "out = pd.DataFrame({'duration_label':Y_test})\n",
    "out.index += 1\n",
    "out.to_csv('output/out_'+'randomforest_gaussiannb_logisticreg'+'.csv',index_label = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "test_data = pd.read_csv('COMP30027_2021_Project2_datasets\\\\input\\\\test_seconds.csv')\n",
    "test_data_100 = pd.read_csv('COMP30027_2021_Project2_datasets\\\\recipe_text_features_doc2vec100\\\\test_steps_doc2vec100.csv',header=None, names = ['coord'+str(i) for i in range(100)])\n",
    "test = pd.concat([test_data,test_data_100],axis=1)\n",
    "x100test = test.drop(['name','steps','ingredients'], axis=1)\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x100)\n",
    "\n",
    "x100test = imp.transform(x100test)\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model.fit(x100,y100)\n",
    "Y_test = model.predict(x100test)\n",
    "\n",
    "out = pd.DataFrame({'duration_label':Y_test})\n",
    "out.index += 1\n",
    "out.to_csv('output/out_'+'GradientBoosting'+'.csv',index_label = 'id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python368jvsc74a57bd04621005da5c26ac209901ca167bf25025457b064ec855aea9ba97365ac8d4984",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}